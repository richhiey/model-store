{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "import os\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#import note_seq\n",
    "#import IPython\n",
    "\n",
    "# Internal imports\n",
    "from data.helpers.midi import MidiEventProcessor\n",
    "from models.midi_transformer import MIDITransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required file paths and common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for local computer\n",
    "# ------------------------------------------------------------------------------------\n",
    "#BASE_DIR = \"/home/richhiey/Desktop/workspace/projects/virtual_musicians\"\n",
    "#DATA_DIR = os.path.join(BASE_DIR, \"data\", \"POP909-Dataset\", \"POP909\")\n",
    "#MIDI_EVENTS_PATH = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909-event-token.npy\")\n",
    "#DATASET_PATH = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909.tfrecords\")\n",
    "#MODEL_SAVE_PATH = os.path.join(BASE_DIR, \"cache\", 'checkpoints', 'model5')\n",
    "#MODEL_CONFIG_PATH = os.path.join(BASE_DIR, 'model-store', 'models', 'configs', 'default.json')\n",
    "\n",
    "## File paths for GPU Container\n",
    "# ------------------------------------------------------------------------------------\n",
    "BASE_DIR          = \"/home/rithomas\"\n",
    "PROJECT_DIR       = os.path.join(BASE_DIR, \"project\")\n",
    "DATA_DIR          = os.path.join(BASE_DIR, \"data\", \"POP909-Dataset\", \"POP909\")\n",
    "MIDI_EVENTS_PATH  = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909-event-token.npy\")\n",
    "DATASET_PATH      = os.path.join(BASE_DIR, \"cache\", \"preprocessed\",  \"pop909.tfrecords\")\n",
    "MODEL_SAVE_PATH   = os.path.join(BASE_DIR, \"cache\", 'checkpoints', 'xl-decoder_2')#\n",
    "MODEL_CONFIG_PATH = os.path.join(PROJECT_DIR, 'model-store', 'models', 'configs', 'default.json')\n",
    "\n",
    "# Common variables\n",
    "# ------------------------------------------------------------------------------------\n",
    "feature_description = {\n",
    "    'melody': tf.io.VarLenFeature(tf.int64),\n",
    "    'rhythm': tf.io.VarLenFeature(tf.int64),\n",
    "    'bridge': tf.io.VarLenFeature(tf.int64)\n",
    "}\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading POP909 MIDI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(909,)\n"
     ]
    }
   ],
   "source": [
    "pop909 = np.load(MIDI_EVENTS_PATH, allow_pickle=True)\n",
    "print(np.shape(pop909))\n",
    "\n",
    "melodies = [song['MELODY'] for song in pop909]\n",
    "rhythms = [song['PIANO'] for song in pop909]\n",
    "bridges = [song['BRIDGE'] for song in pop909]\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    with tf.io.TFRecordWriter(DATASET_PATH) as file_writer:\n",
    "        for melody, rhythm, bridge in zip(melodies, rhythms, bridges):\n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                    feature={\n",
    "                        \"melody\": tf.train.Feature(int64_list=tf.train.Int64List(value=[389]+melody+[390])),\n",
    "                        \"rhythm\": tf.train.Feature(int64_list=tf.train.Int64List(value=[389]+rhythm+[390])),\n",
    "                        \"bridge\": tf.train.Feature(int64_list=tf.train.Int64List(value=[389]+bridge+[390]))\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            file_writer.write(example.SerializeToString())\n",
    "        file_writer.close()\n",
    "\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(DATASET_PATH)\n",
    "dataset = raw_dataset.map(_parse_function).batch(BATCH_SIZE, drop_remainder=True).repeat(1000)\n",
    "event_processor = MidiEventProcessor()\n",
    "piano = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
    "\n",
    "# Reconstruct MIDI data from TFRecord\n",
    "#for i, data in enumerate(dataset.take(3)):\n",
    "#    print('----------------------------------------------------------')\n",
    "#    full_midi = pretty_midi.PrettyMIDI()\n",
    "    \n",
    "#    melody_instr = pretty_midi.Instrument(program=piano)\n",
    "#    rhythm_instr = pretty_midi.Instrument(program=piano)\n",
    "#    bridge_instr = pretty_midi.Instrument(program=piano)\n",
    "    \n",
    "#    print('Melody:')\n",
    "#    melody_events = tf.sparse.to_dense(data['melody']).numpy()\n",
    "#    print(melody_events)\n",
    "#    for note in event_processor.decode(melody_events):\n",
    "#        melody_instr.notes.append(note)\n",
    "    \n",
    "#    print('Rhythm:')\n",
    "#    rhythm_events = tf.sparse.to_dense(data['rhythm']).numpy()\n",
    "#    print(rhythm_events)\n",
    "#    for note in event_processor.decode(rhythm_events):\n",
    "#        rhythm_instr.notes.append(note)\n",
    "    \n",
    "#    print('Bridge:')\n",
    "#    bridge_events = tf.sparse.to_dense(data['bridge']).numpy()\n",
    "#    print(bridge_events)\n",
    "#    for note in event_processor.decode(bridge_events):\n",
    "#        bridge_instr.notes.append(note)\n",
    "    \n",
    "#    full_midi.instruments.append(melody_instr)\n",
    "#    full_midi.instruments.append(rhythm_instr)\n",
    "#    full_midi.instruments.append(bridge_instr)\n",
    "#    IPython.display.display(IPython.display.Audio(full_midi.fluidsynth(), rate=44100))\n",
    "#    filename = 'test_'+str(i)+'.mid'\n",
    "#    full_midi.write(filename)\n",
    "    \n",
    "#    full_midi_ns = note_seq.midi_io.midi_file_to_note_sequence(filename)\n",
    "#    note_seq.plot_sequence(full_midi_ns)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer XL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RestoredDECODERfrom /home/rithomas/cache/checkpoints/xl-decoder_2/ckpt/decoder/ckpt-2\n",
      "<models.midi_transformer.MIDITransformer object at 0x7fddc017d208>\n",
      "<RepeatDataset shapes: {bridge: (16, None), melody: (16, None), rhythm: (16, None)}, types: {bridge: tf.int64, melody: tf.int64, rhythm: tf.int64}>\n",
      "Training the encoder on key: melody\n",
      "Training the encoder on key: melody\n",
      "WARNING:tensorflow:From /home/rithomas/project/venvs/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:08,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets listen to what to the model sounds like step 0!\n",
      "Loss (0) - tf.Tensor(5.9686456, shape=(), dtype=float32)\n",
      "Predicted by model:\n",
      "tf.Tensor(\n",
      "[[133 133 133 ...   0   0   0]\n",
      " [133 133 202 ...   0   0   0]\n",
      " [133 133 202 ...   0   0   0]\n",
      " ...\n",
      " [133 133 133 ...   0   0   0]\n",
      " [311 311 311 ...   0   0   0]\n",
      " [133 133 202 ...   0   0   0]], shape=(16, 2205), dtype=int64)\n",
      "Real Music:\n",
      "tf.Tensor(\n",
      "[[389 355 355 ...   0   0   0]\n",
      " [389 355 355 ...   0   0   0]\n",
      " [389 355 355 ...   0   0   0]\n",
      " ...\n",
      " [389 355 355 ...   0   0   0]\n",
      " [389 355 355 ...   0   0   0]\n",
      " [389 355 355 ...   0   0   0]], shape=(16, 2205), dtype=int64)\n",
      "Saved checkpoint for step 1: /home/rithomas/cache/checkpoints/xl-decoder_2/ckpt/decoder/ckpt-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [03:05,  7.63s/it]"
     ]
    }
   ],
   "source": [
    "midi_transformer = MIDITransformer(MODEL_CONFIG_PATH, MODEL_SAVE_PATH)\n",
    "midi_transformer.reset_states()\n",
    "print(midi_transformer)\n",
    "print(dataset)\n",
    "midi_transformer.train(dataset, {'inputs': 'melody', 'outputs': 'melody', 'num_epochs': '1000'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new Note Sequences with the MIDI Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#midi_transformer.predict(midi_note_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_music_env",
   "language": "python",
   "name": "ai_music_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
