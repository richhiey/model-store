{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "import os\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#import note_seq\n",
    "#import IPython\n",
    "\n",
    "# Internal imports\n",
    "from data.helpers.midi import MidiEventProcessor\n",
    "from models.midi_transformer import MIDITransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required file paths and common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for local computer\n",
    "# ------------------------------------------------------------------------------------\n",
    "BASE_DIR = \"/home/richhiey/Desktop/workspace/projects/virtual_musicians\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"POP909-Dataset\", \"POP909\")\n",
    "MIDI_EVENTS_PATH = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909-event-token.npy\")\n",
    "DATASET_PATH = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909.tfrecords\")\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_DIR, \"cache\", 'checkpoints', 'model5')\n",
    "MODEL_CONFIG_PATH = os.path.join(BASE_DIR, 'model-store', 'models', 'configs', 'default.json')\n",
    "\n",
    "## File paths for GPU Container\n",
    "# ------------------------------------------------------------------------------------\n",
    "#BASE_DIR          = \"/home/rithomas\"\n",
    "#PROJECT_DIR       = os.path.join(BASE_DIR, \"project\")\n",
    "#DATA_DIR          = os.path.join(BASE_DIR, \"data\", \"POP909-Dataset\", \"POP909\")\n",
    "#MIDI_EVENTS_PATH  = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909-event-token.npy\")\n",
    "#DATASET_PATH      = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909.tfrecords\")\n",
    "#MODEL_SAVE_PATH   = os.path.join(BASE_DIR, \"cache\", 'checkpoints', 'test_model')#\n",
    "#MODEL_CONFIG_PATH = os.path.join(PROJECT_DIR, 'model-store', 'models', 'configs', 'default.json')\n",
    "\n",
    "# Common variables\n",
    "# ------------------------------------------------------------------------------------\n",
    "feature_description = {\n",
    "    'melody': tf.io.VarLenFeature(tf.int64),\n",
    "    'rhythm': tf.io.VarLenFeature(tf.int64),\n",
    "    'bridge': tf.io.VarLenFeature(tf.int64)\n",
    "}\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading POP909 MIDI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(909,)\n"
     ]
    }
   ],
   "source": [
    "pop909 = np.load(MIDI_EVENTS_PATH, allow_pickle=True)\n",
    "print(np.shape(pop909))\n",
    "\n",
    "melodies = [song['MELODY'] for song in pop909]\n",
    "rhythms = [song['PIANO'] for song in pop909]\n",
    "bridges = [song['BRIDGE'] for song in pop909]\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    with tf.io.TFRecordWriter(DATASET_PATH) as file_writer:\n",
    "        for melody, rhythm, bridge in zip(melodies, rhythms, bridges):\n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                    feature={\n",
    "                        \"melody\": tf.train.Feature(int64_list=tf.train.Int64List(value=melody)),\n",
    "                        \"rhythm\": tf.train.Feature(int64_list=tf.train.Int64List(value=rhythm)),\n",
    "                        \"bridge\": tf.train.Feature(int64_list=tf.train.Int64List(value=bridge))\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            file_writer.write(example.SerializeToString())\n",
    "        file_writer.close()\n",
    "\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(DATASET_PATH)\n",
    "dataset = raw_dataset.map(_parse_function).batch(BATCH_SIZE, drop_remainder=True).repeat(1000)\n",
    "event_processor = MidiEventProcessor()\n",
    "piano = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
    "\n",
    "# Reconstruct MIDI data from TFRecord\n",
    "#for i, data in enumerate(dataset.take(3)):\n",
    "#    print('----------------------------------------------------------')\n",
    "#    full_midi = pretty_midi.PrettyMIDI()\n",
    "    \n",
    "#    melody_instr = pretty_midi.Instrument(program=piano)\n",
    "#    rhythm_instr = pretty_midi.Instrument(program=piano)\n",
    "#    bridge_instr = pretty_midi.Instrument(program=piano)\n",
    "    \n",
    "#    print('Melody:')\n",
    "#    melody_events = tf.sparse.to_dense(data['melody']).numpy()\n",
    "#    print(melody_events)\n",
    "#    for note in event_processor.decode(melody_events):\n",
    "#        melody_instr.notes.append(note)\n",
    "    \n",
    "#    print('Rhythm:')\n",
    "#    rhythm_events = tf.sparse.to_dense(data['rhythm']).numpy()\n",
    "#    print(rhythm_events)\n",
    "#    for note in event_processor.decode(rhythm_events):\n",
    "#        rhythm_instr.notes.append(note)\n",
    "    \n",
    "#    print('Bridge:')\n",
    "#    bridge_events = tf.sparse.to_dense(data['bridge']).numpy()\n",
    "#    print(bridge_events)\n",
    "#    for note in event_processor.decode(bridge_events):\n",
    "#        bridge_instr.notes.append(note)\n",
    "    \n",
    "#    full_midi.instruments.append(melody_instr)\n",
    "#    full_midi.instruments.append(rhythm_instr)\n",
    "#    full_midi.instruments.append(bridge_instr)\n",
    "#    IPython.display.display(IPython.display.Audio(full_midi.fluidsynth(), rate=44100))\n",
    "#    filename = 'test_'+str(i)+'.mid'\n",
    "#    full_midi.write(filename)\n",
    "    \n",
    "#    full_midi_ns = note_seq.midi_io.midi_file_to_note_sequence(filename)\n",
    "#    note_seq.plot_sequence(full_midi_ns)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer XL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RestoredDECODERfrom /home/richhiey/Desktop/workspace/projects/virtual_musicians/cache/checkpoints/model5/ckpt/decoder/ckpt-2\n",
      "<models.midi_transformer.MIDITransformer object at 0x7f4e74661400>\n",
      "<RepeatDataset shapes: {bridge: (2, None), melody: (2, None), rhythm: (2, None)}, types: {bridge: tf.int64, melody: tf.int64, rhythm: tf.int64}>\n",
      "Training the encoder on key: melody\n",
      "Training the encoder on key: melody\n",
      "WARNING:tensorflow:From /home/richhiey/Desktop/workspace/projects/virtual_musicians/model-store/models/helpers/layers.py:80: calling Layer.add_update (from tensorflow.python.keras.engine.base_layer) with inputs is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`inputs` is now automatically inferred\n",
      "WARNING:tensorflow:From /home/richhiey/.local/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "Lets listen to what to the model sounds like step 0!\n",
      "Loss (0) - tf.Tensor(5.963916, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[-1.0413721  -0.22675394 -0.5715836  ...  1.5771728   1.8969318\n",
      "    1.458142  ]\n",
      "  [-0.9358003  -0.20579258 -0.5142841  ...  1.5611439   2.0560727\n",
      "    1.4523529 ]\n",
      "  [-0.99315405 -0.14918093 -0.4834387  ...  1.556137    1.8644084\n",
      "    1.4040723 ]\n",
      "  ...\n",
      "  [-1.0543255  -0.19006944 -0.729211   ...  1.5885787   1.8944024\n",
      "    1.48565   ]\n",
      "  [-1.0535918  -0.19062018 -0.7287221  ...  1.5901035   1.8957112\n",
      "    1.4884856 ]\n",
      "  [-1.0535915  -0.1905598  -0.727794   ...  1.5919471   1.8973176\n",
      "    1.4881468 ]]\n",
      "\n",
      " [[-0.8977525  -0.12935105 -0.56080794 ...  1.2598739   1.6968786\n",
      "    1.6307621 ]\n",
      "  [-0.89763284 -0.12937824 -0.56083465 ...  1.25984     1.6968578\n",
      "    1.6307905 ]\n",
      "  [-0.89753026 -0.12943567 -0.5608166  ...  1.2598547   1.6968629\n",
      "    1.6308644 ]\n",
      "  ...\n",
      "  [-0.8981498  -0.12888353 -0.56084275 ...  1.2585651   1.6963278\n",
      "    1.6301461 ]\n",
      "  [-0.8979939  -0.1290914  -0.56103283 ...  1.2586739   1.6965786\n",
      "    1.6300768 ]\n",
      "  [-0.8978811  -0.12927745 -0.5611698  ...  1.2587863   1.6967292\n",
      "    1.6300094 ]]], shape=(2, 255, 256), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:19, 19.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 1: /home/richhiey/Desktop/workspace/projects/virtual_musicians/cache/checkpoints/model5/ckpt/decoder/ckpt-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [07:08,  7.22s/it]"
     ]
    }
   ],
   "source": [
    "midi_transformer = MIDITransformer(MODEL_CONFIG_PATH, MODEL_SAVE_PATH)\n",
    "midi_transformer.reset_states()\n",
    "print(midi_transformer)\n",
    "print(dataset)\n",
    "midi_transformer.train(dataset, {'inputs': 'melody', 'outputs': 'melody', 'num_epochs': '1000'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new Note Sequences with the MIDI Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#midi_transformer.predict(midi_note_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_store_venv",
   "language": "python",
   "name": "model_store_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
