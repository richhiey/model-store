{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "import os\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#import note_seq\n",
    "#import IPython\n",
    "\n",
    "# Internal imports\n",
    "from data.helpers.midi import MidiEventProcessor\n",
    "from models.midi_transformer import MIDITransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required file paths and common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for local computer\n",
    "# ------------------------------------------------------------------------------------\n",
    "BASE_DIR = \"/home/richhiey/Desktop/workspace/projects/virtual_musicians\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"POP909-Dataset\", \"POP909\")\n",
    "MIDI_EVENTS_PATH = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909-event-token.npy\")\n",
    "DATASET_PATH = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909.tfrecords\")\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_DIR, \"cache\", 'checkpoints', 'model3')\n",
    "MODEL_CONFIG_PATH = os.path.join(BASE_DIR, 'model-store', 'models', 'configs', 'default.json')\n",
    "\n",
    "## File paths for GPU Container\n",
    "# ------------------------------------------------------------------------------------\n",
    "# BASE_DIR          = \"/home/rithomas\"\n",
    "# PROJECT_DIR       = os.path.join(BASE_DIR, \"project\")\n",
    "# DATA_DIR          = os.path.join(BASE_DIR, \"data\", \"POP909-Dataset\", \"POP909\")\n",
    "# MIDI_EVENTS_PATH  = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909-event-token.npy\")\n",
    "# DATASET_PATH      = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909.tfrecords\")\n",
    "# MODEL_SAVE_PATH   = os.path.join(BASE_DIR, \"cache\", 'checkpoints', 'test_model')\n",
    "# MODEL_CONFIG_PATH = os.path.join(PROJECT_DIR, 'model-store', 'models', 'configs', 'default.json')\n",
    "\n",
    "# Common variables\n",
    "# ------------------------------------------------------------------------------------\n",
    "feature_description = {\n",
    "    'melody': tf.io.VarLenFeature(tf.int64),\n",
    "    'rhythm': tf.io.VarLenFeature(tf.int64),\n",
    "    'bridge': tf.io.VarLenFeature(tf.int64)\n",
    "}\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading POP909 MIDI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop909 = np.load(MIDI_EVENTS_PATH, allow_pickle=True)\n",
    "print(np.shape(pop909))\n",
    "\n",
    "melodies = [song['MELODY'] for song in pop909]\n",
    "rhythms = [song['PIANO'] for song in pop909]\n",
    "bridges = [song['BRIDGE'] for song in pop909]\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    with tf.io.TFRecordWriter(DATASET_PATH) as file_writer:\n",
    "        for melody, rhythm, bridge in zip(melodies, rhythms, bridges):\n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                    feature={\n",
    "                        \"melody\": tf.train.Feature(int64_list=tf.train.Int64List(value=melody)),\n",
    "                        \"rhythm\": tf.train.Feature(int64_list=tf.train.Int64List(value=rhythm)),\n",
    "                        \"bridge\": tf.train.Feature(int64_list=tf.train.Int64List(value=bridge))\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            file_writer.write(example.SerializeToString())\n",
    "        file_writer.close()\n",
    "\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(DATASET_PATH)\n",
    "dataset = raw_dataset.map(_parse_function).batch(BATCH_SIZE)\n",
    "event_processor = MidiEventProcessor()\n",
    "piano = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
    "\n",
    "# Reconstruct MIDI data from TFRecord\n",
    "#for i, data in enumerate(dataset.take(3)):\n",
    "#    print('----------------------------------------------------------')\n",
    "#    full_midi = pretty_midi.PrettyMIDI()\n",
    "    \n",
    "#    melody_instr = pretty_midi.Instrument(program=piano)\n",
    "#    rhythm_instr = pretty_midi.Instrument(program=piano)\n",
    "#    bridge_instr = pretty_midi.Instrument(program=piano)\n",
    "    \n",
    "#    print('Melody:')\n",
    "#    melody_events = tf.sparse.to_dense(data['melody']).numpy()\n",
    "#    print(melody_events)\n",
    "#    for note in event_processor.decode(melody_events):\n",
    "#        melody_instr.notes.append(note)\n",
    "    \n",
    "#    print('Rhythm:')\n",
    "#    rhythm_events = tf.sparse.to_dense(data['rhythm']).numpy()\n",
    "#    print(rhythm_events)\n",
    "#    for note in event_processor.decode(rhythm_events):\n",
    "#        rhythm_instr.notes.append(note)\n",
    "    \n",
    "#    print('Bridge:')\n",
    "#    bridge_events = tf.sparse.to_dense(data['bridge']).numpy()\n",
    "#    print(bridge_events)\n",
    "#    for note in event_processor.decode(bridge_events):\n",
    "#        bridge_instr.notes.append(note)\n",
    "    \n",
    "#    full_midi.instruments.append(melody_instr)\n",
    "#    full_midi.instruments.append(rhythm_instr)\n",
    "#    full_midi.instruments.append(bridge_instr)\n",
    "#    IPython.display.display(IPython.display.Audio(full_midi.fluidsynth(), rate=44100))\n",
    "#    filename = 'test_'+str(i)+'.mid'\n",
    "#    full_midi.write(filename)\n",
    "    \n",
    "#    full_midi_ns = note_seq.midi_io.midi_file_to_note_sequence(filename)\n",
    "#    note_seq.plot_sequence(full_midi_ns)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer XL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_transformer = MIDITransformer(MODEL_CONFIG_PATH, MODEL_SAVE_PATH)\n",
    "midi_transformer.reset_states()\n",
    "print(midi_transformer)\n",
    "print(dataset)\n",
    "midi_transformer.train(dataset, {'inputs': 'melody', 'outputs': 'melody', 'num_epochs': '1000'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new Note Sequences with the MIDI Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#midi_transformer.predict(midi_note_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_store_venv",
   "language": "python",
   "name": "model_store_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
