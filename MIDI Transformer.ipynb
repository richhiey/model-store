{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "import os\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import note_seq\n",
    "import IPython\n",
    "\n",
    "# Internal imports\n",
    "from data.helpers.midi import MidiEventProcessor\n",
    "from models.midi_transformer import MIDITransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required file paths and common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File paths\n",
    "BASE_DIR = \"/home/richhiey/Desktop/workspace/projects/virtual_musicians\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"POP909-Dataset\", \"POP909\")\n",
    "MIDI_EVENTS_PATH = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909-event-token.npy\")\n",
    "DATASET_PATH = os.path.join(BASE_DIR, \"data\", \"preprocessed\",  \"pop909.tfrecords\")\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_DIR, \"cache\", 'checkpoints', 'model3')\n",
    "MODEL_CONFIG_PATH = os.path.join(BASE_DIR, 'model-store', 'models', 'configs', 'default.json')\n",
    "\n",
    "# Common variables\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading POP909 MIDI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(909,)\n"
     ]
    }
   ],
   "source": [
    "pop909 = np.load(MIDI_EVENTS_PATH, allow_pickle=True)\n",
    "print(np.shape(pop909))\n",
    "\n",
    "melodies = [song['MELODY'] for song in pop909]\n",
    "rhythms = [song['PIANO'] for song in pop909]\n",
    "bridges = [song['BRIDGE'] for song in pop909]\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    with tf.io.TFRecordWriter(DATASET_PATH) as file_writer:\n",
    "        for melody, rhythm, bridge in zip(melodies, rhythms, bridges):\n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                    feature={\n",
    "                        \"melody\": tf.train.Feature(int64_list=tf.train.Int64List(value=melody)),\n",
    "                        \"rhythm\": tf.train.Feature(int64_list=tf.train.Int64List(value=rhythm)),\n",
    "                        \"bridge\": tf.train.Feature(int64_list=tf.train.Int64List(value=bridge))\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            file_writer.write(example.SerializeToString())\n",
    "\n",
    "        file_writer.close()\n",
    "    \n",
    "\n",
    "feature_description = {\n",
    "    'melody': tf.io.VarLenFeature(tf.int64),\n",
    "    'rhythm': tf.io.VarLenFeature(tf.int64),\n",
    "    'bridge': tf.io.VarLenFeature(tf.int64)\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(DATASET_PATH)\n",
    "dataset = raw_dataset.map(_parse_function).batch(BATCH_SIZE)\n",
    "event_processor = MidiEventProcessor()\n",
    "piano = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
    "\n",
    "# Reconstruct MIDI data from TFRecord\n",
    "#for i, data in enumerate(dataset.take(3)):\n",
    "#    print('----------------------------------------------------------')\n",
    "#    full_midi = pretty_midi.PrettyMIDI()\n",
    "    \n",
    "#    melody_instr = pretty_midi.Instrument(program=piano)\n",
    "#    rhythm_instr = pretty_midi.Instrument(program=piano)\n",
    "#    bridge_instr = pretty_midi.Instrument(program=piano)\n",
    "    \n",
    "#    print('Melody:')\n",
    "#    melody_events = tf.sparse.to_dense(data['melody']).numpy()\n",
    "#    print(melody_events)\n",
    "#    for note in event_processor.decode(melody_events):\n",
    "#        melody_instr.notes.append(note)\n",
    "    \n",
    "#    print('Rhythm:')\n",
    "#    rhythm_events = tf.sparse.to_dense(data['rhythm']).numpy()\n",
    "#    print(rhythm_events)\n",
    "#    for note in event_processor.decode(rhythm_events):\n",
    "#        rhythm_instr.notes.append(note)\n",
    "    \n",
    "#    print('Bridge:')\n",
    "#    bridge_events = tf.sparse.to_dense(data['bridge']).numpy()\n",
    "#    print(bridge_events)\n",
    "#    for note in event_processor.decode(bridge_events):\n",
    "#        bridge_instr.notes.append(note)\n",
    "    \n",
    "#    full_midi.instruments.append(melody_instr)\n",
    "#    full_midi.instruments.append(rhythm_instr)\n",
    "#    full_midi.instruments.append(bridge_instr)\n",
    "#    IPython.display.display(IPython.display.Audio(full_midi.fluidsynth(), rate=44100))\n",
    "#    filename = 'test_'+str(i)+'.mid'\n",
    "#    full_midi.write(filename)\n",
    "    \n",
    "#    full_midi_ns = note_seq.midi_io.midi_file_to_note_sequence(filename)\n",
    "#    note_seq.plot_sequence(full_midi_ns)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer XL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored Encoder from /home/richhiey/Desktop/workspace/projects/virtual_musicians/cache/checkpoints/model3/encoder/ckpt/ckpt-4\n",
      "Restored Decoder from /home/richhiey/Desktop/workspace/projects/virtual_musicians/cache/checkpoints/model3/decoder/ckpt/ckpt-4\n",
      "<models.midi_transformer.MIDITransformer object at 0x7fb0402e0d00>\n",
      "<BatchDataset shapes: {bridge: (None, None), melody: (None, None), rhythm: (None, None)}, types: {bridge: tf.int64, melody: tf.int64, rhythm: tf.int64}>\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.963599, shape=(), dtype=float32)\n",
      "Saved checkpoint for step 6: /home/richhiey/Desktop/workspace/projects/virtual_musicians/cache/checkpoints/model3/encoder/ckpt/ckpt-5, /home/richhiey/Desktop/workspace/projects/virtual_musicians/cache/checkpoints/model3/decoder/ckpt/ckpt-5\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.9636307, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.963516, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.9636025, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.963374, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.9633574, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.963005, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.963133, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.962889, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.9631305, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_2/kernel_r:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_q:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_kv:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_o:0', 'transformer_xl_decoder_stack/xl_decoder_layer/relative_partial_multi_head_self_attention_3/kernel_r:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_4/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/gamma:0', 'transformer_xl_decoder_stack/xl_decoder_layer/layer_normalization_5/beta:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_context:0', 'transformer_xl_decoder_stack/xl_decoder_layer/RelativeBiasLayer-1/bias_relative:0'] when minimizing the loss.\n",
      "tf.Tensor(5.962873, shape=(), dtype=float32)\n",
      "Encoder part\n",
      "-------------------------------------------------------------------\n",
      "Decoder part\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e4f42bd9dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_transformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmidi_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/workspace/projects/virtual_musicians/model-store/models/midi_transformer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, train_configs)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0mrhythm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msong\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rhythm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;31m## -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhythm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;31m## -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/projects/virtual_musicians/model-store/models/midi_transformer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mflat_list_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_vars\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m## -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_list_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;31m#print(gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m## -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1065\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1068\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_aggregate_grads\u001b[0;34m(gradients)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     assert all(isinstance(g, (ops.Tensor, ops.IndexedSlices))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "midi_transformer = MIDITransformer(MODEL_CONFIG_PATH, MODEL_SAVE_PATH)\n",
    "midi_transformer.reset_states()\n",
    "print(midi_transformer)\n",
    "print(dataset)\n",
    "midi_transformer.train(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new Note Sequences with the MIDI Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#midi_transformer.predict(midi_note_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_store_venv",
   "language": "python",
   "name": "model_store_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
